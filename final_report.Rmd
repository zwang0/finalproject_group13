---
title: "Final Report"
author: "Group 13"
date: "2021/12/16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
Bike-Sharing systems are currently widely introduced in urban cities to solve
the “last mile” problem, improve the link between other modes of transportation.
Bike-sharing systems facilitate the use of public transportation, enhance
traffic troubles, as well as minimize greenhouse gas emissions. However, the
availability and accessibility of sharing bikes could be a problem since the
demand and supply of bikes are not stable.

This project aims to use machine learning and data-mining-based  algorithms to
predict the demand for rental bikes in Seoul city at each hour in order to
provide a stable bike supply. The project is based on the dataset downloaded at
UC Irvine Machine Learning Repository. The dataset contains the hourly public
rental number of Seoul bikes with date and weather information (Temperature,
Humidity, Wind speed, Visibility, Dew point, Solar radiation, Snowfall,
Rainfall) for one year from December 2017 to November 2018, 365 days. The number
of rental bikes rented at every hour is determined from the bike rental history
data collected from the Seoul Public Data Park website of South Korea.[1]

As all modes of transportation depend primarily on the weather conditions,
including cycling, we would assume that the corresponding climate conditions
have effects on the total number of rental bikes rented at each hour. And we
would also assume that date parameters such as weekdays may enhance the
performance of the prediction model. Thus, weather details and date parameters
would be the covariates of the model.

## Methodology
### Linear Regression
Linear Regression Model (LM) is the simplest method to analyze the relationship
of outcome Y and predictor X. The model is defined as below.
\begin{align}
Y=\beta_0 + X^T\beta
\end{align}
In the regression, after we input the data of X and Y, the coefficients
$\beta_0$ and $\beta$ could be estimated by solving the equation.

### LASSO
### Ridge
### Bagged Forest
### Random Forest
### Support Vector Machine
Support Vector Machine(SVM) was designed for classification, and later the idea
was generalized to regression. The optimization idea in SVM classification is to
increase the amount of separation between two classes. However, in SVM
regression, we would like to form a "band" around the true regression function
that contains most of the points. With this purpose, the loss function is
defined as below.
\begin{align}
\mu(x)=\beta_0 + x^T\beta
\end{align}
If the point (X,Y) = (x,y) is such that $|y-\mu(x)|\leq \epsilon$, then the loss
is taken to be zero; if $|y-\mu(x)|\geq \epsilon$, then the loss is taken to be
$|y-\mu(x)|- \epsilon$. The main goal is to find a $\mu(x)$ such that most
points with the loss taken to be zero.

### Neural Network
Neural networks (NNs), also known as artificial neural networks (ANNs), are
computing systems inspired by the way biological neural networks in the human
brain process information. Building Neural networks is a widely adopted method
in machine learning. The tasks to which neural networks are applied tend to fall
within the following broad categories including regression analysis,
classification, and data processing. Neural networks provide the current best
solutions in solving many computer science problems such as image recognition,
speech recognition, and natural language processing.

In this project, we are going to solve a regression problem and make a
prediction using neural networks. Instead of applying Convolutional neural
network or Recurrent neural network, we adopt the most basic one: 
*Feedforward Neural Network* (FFNN). Feedforward neural network was the first
type of artificial neural network devised. Different from recurrent neural
networks, the connections between nodes do not form a circle. It has only one
direction from input layer formed by input nodes, through hidden layers and to
out the output layer formed by output nodes. Specifically, we will adopt
*Multilayer Perceptron* (MLP). A multilayer perceptron consists of at least
three layers of nodes: an input layer, a hidden layer and an output layer. There
are 8 hidden layers in our model.

The toolkit we used for building the neural network is *Keras*. Keras is a deep
learning API (application programming interface) written in Python, running on
top of the machine learning platform *TensorFlow*. The activation function we
adopted are ReLU and Linear. Rectified Linear Unit (ReLU) is an activation
function that defines the positive part of its arguement. It can be expressed
as:
$$f(x) = x^{+} = \max(0, x)$$
Linear is an activation function that simply returns the the argument itself. We
also add regularizers that apply to a L1 and L2 penalty on the layer's output.
L1 penalty is the penalty term in L1 regression or Lasso regression. L2 penalty
is the penalty term in L2 regression or Ridge regression. Regularizers are used 
to avoid model overfitting and improve the robustness and generality. 

## Dataset description
Data description

The raw data is from the UCI machine learning repository which is a collection of 8,760 renting records from 2017 to 2018. There are 14 variables in the dataset. The table shows the features of each variable. The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.

Exploratory data analysis

The data set included in the UCI we used has been processed, so there is no missing data. What we have done is to extract day, month, year from date, and convert date to the day of week. The final rental bike data is partitioned into two namely, training set for building the regression and testing set for assessing the model performance. In a random way, 20% of the 8760 records were selected as the test set and the other 80% as the data of the training set. 

Fig 1. shows the total number of rented bikes for the entire period. It illustrates that the rental bike count is highly variable at each hour. As can be seen, there is a long tail in the data distribution.
```{r}
  ggplot(bike_data, aes(x=Rented_Bike_Count)) +
    geom_histogram(binwidth=110,
                   fill="lightblue", 
                   col="orange",
                   alpha = .8)+
  labs(title="Fig 1. Histogram for Rented Bike Count", x="Rented Bike Count", y="Count")+
  theme_stata() +
  scale_color_stata() +
  theme(plot.title = element_text(hjust = 0.5),legend.position = "None")
```
Fig 2. displays an average number of the day throughout the week. It is shown that the count distribution follows identical trends over the weekdays and different patterns over the weekends. Fig 3. shows that the average count is high at each hour in the summer and low in the winter. The count is quite similar during autumn and spring. The count abruptly increases in hours 8 and 18. This is because the hours from 8 AM and 6 PM is regarded as the peak hours, during which the usage of the rental bike is high in Seoul.
```{r}
# Plot Average count by week 
ggplot(bike_data, aes(x=DWeek, y=Rented_Bike_Count ,fill= DWeek))+
  geom_bar(stat = "identity") +
  ggtitle("Fig 2. Average Count by Week")+
  xlab("Day of Week") + 
  ylab("Rent count")  + 
  labs(fill = "Day of Week") + 
  theme_stata() +
  scale_color_stata() +
  theme(plot.title = element_text(hjust = 0.5),legend.position = "None")
# Plot Average count by month
ggplot(bike_data, aes(x=Month, y=Rented_Bike_Count ,fill= Month))+
  geom_bar(stat = "identity") +
  ggtitle("Fig 3. Average Count by Month")+
  xlab("Month") + 
  ylab("Rent count")  + 
  labs(fill = "Month") + 
  theme_stata() +
  scale_color_stata() +
  theme(plot.title = element_text(hjust = 0.5),legend.position = "None",axis.text.x = element_text(angle=-25))
```

As shown in Fig.(), the corplot function creates a graph of a correlation matrix, coloring the regions according to the value correlation coefficients. A correlation value of 1 is considered as a total positive correlation, −1 is considered total negative correlation, and if 0 no correlation exists between the variable. Positive correlations are notable between count and hour, temp, wind, visb, dew and solar. There is a negative correlation between count and humidity, rain and snow respectively. These correlation values imply that the weather variables are related to the rental bike count at each hour. 
```{r }
#plot correlation
library(corrplot)
library(RColorBrewer)
my_data <- bike_data[, 2:11]
colnames(my_data)[c(1,3:10)] <- c('Count', 'Temp', 'Humiditity', 'Wind', 'Visb', 'Dew', 'Solar','Rain','Snow')
M <-cor(my_data)
corrplot(M, method = "color", type = "upper", 
          addCoef.col = "black",number.cex=0.75, tl.col = "darkblue",
         diag = FALSE,col=brewer.pal(n=8, name="RdYlBu"), tl.srt = 30, 
         title="Fig Correlation Plot" ,
          mar=c(0,0,1,0))
```

## Evaluation index
The performance assessment index used here is Root Mean Squared Error (RMSE), which is the standard sample deviation between the observed and the predicted values of the  residuals. RMSE measures the fluctuation of variance regarding different models. The better model is the model with lower RMSE. RMSE is defined by the equation as below.
\begin{align}
RMSE=\sqrt{\frac{\sum^n_{i=1}(Y_i-\hat{Y_i})^2}{n}}
\end{align}

## Model development
It is essential to tuning parameters in order to find the optimal model with
relatively low loss or error values.

When building neural networks, we have tried different size of the hidden
layers. Greater number of nodes in the layer indicates more parameters would be
used for training and model fitting. We believe that it is more likely to fit a
model with more parameters. Hence we add the hidden layers from 5, the starting
model, to 8, the final optimal model. We also try different number of nodes
ranging from 1 to 512 in different layers. There are more nodes in the first few
hidden layers, and the nodes number keep reducing until the size of output
(which is 1 since we only need to make a prediction of the number of rented
bike).

However, large number of parameters may also bring the problem: overfit. To
solve this problem, we constraint the complexity of the model. No more than 8
hidden layers and no more than 512 number of nodes in each layers. Typically, we
adopt 256 or 64 nodes per layer. Futhermore, we introduce regularizers that add
L1 or L2 penalty to the model during training base on the magnitude of the
activation. We also try to randomly dropout a portion (i.e. 20%) of the input in
specific layer while training. But later we find that the performance of these 
models are not as good as the one without dropping input.


## Result

 Method | Training | Testing
   -    |    -     |   -
 LM     | 407.0000 | 417.0000
 Ridge  | 427.1653 | 437.7671
 LASSO  | 427.3688 | 437.4730
 RF     | 224.6018 | 222.9583
 BagF   | 184.1149 | 182.7463
 SVM    | 226.5303 | 221.5075
 NN     | 120.0056 | 133.2593

## Discussion

## Citation

E, S., Park, J., &amp; Cho, Y. (2020). Using data mining techniques for bike
sharing demand prediction in Metropolitan City. Computer Communications, 153,
353–366. https://doi.org/10.1016/j.comcom.2020.02.007
