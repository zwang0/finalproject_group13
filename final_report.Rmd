---
title: Bike Sharing Demand Prediction in Seoul
author: Zehua Wang, Ziman Jiang, Faye Fang, Yue Pan
date: December 17, 2021
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction
Bike-Sharing systems are currently being widely introduced in urban areas to
solve the “last mile” problem, i.e. improve the link between other modes of
transportation. Bike-sharing systems facilitate the use of public
transportation, enhance traffic troubles, as well as minimize greenhouse gas
emissions. However, the availability and accessibility of sharing bikes could be
a problem since the demand and supply of bikes are not stable.

This project aims to use machine learning and data-mining-based  algorithms to
predict the demand for rental bikes in Seoul City at each hour in order to
provide a stable bike supply. The project is based on a dataset downloaded from
UC Irvine Machine Learning Repository. The dataset contains the hourly public
rental number of Seoul bikes with date and weather information (Temperature,
Humidity, Wind speed, Visibility, Dew point, Solar radiation, Snowfall,
Rainfall) for one year from December 2017 to November 2018. The number of rental
bikes rented at every hour is determined from the bike rental history data
collected from the Seoul Public Data Park website of South Korea.[1]

As all modes of transportation depend primarily on the weather conditions,
including cycling, we assume that the corresponding climate conditions affect
the total number of rental bikes rented at each hour. We also assume that date
parameters such as weekdays may enhance the performance of the prediction model.
Thus, weather details and date parameters are the covariates of the model.

In this paper, we compare compare different methods to see which one makes the
best prediction of the rental bike number measured by root mean squared error.

## Methodology
### Linear Regression
Linear Regression Model (LM) is the simplest method to analyze the relationship
of outcome Y and predictor X. The model is defined as below.
\begin{align}
Y=\beta_0 + X^T\beta
\end{align}
In the regression, the coefficients $\beta_0$ and $\beta$ could be estimated by 
solving the equation.

### Ridge and LASSO
Ridge and LASSO regression estimates coefficients of multiple linear regression
with the aim for lower variance. By minimizing the shrinkage penalty(
$\lambda\Sigma_{j=1}^k |\hat{\beta_j^2}|$ for Ridge and 
$\lambda \Sigma_{j=1}^k|\hat{\beta_j}|$ for LASSO) along with RSS, unnecessary
estimated coefficients will be shrunk. The coefficient $\lambda$ controls the
magnitude of the penalty. As it increases, coefficients of  Ridge tend towards 0
and coefficients of LASSO shrunk to absolute zero, leading to low variance and
low bias. The major difference between two methods is that LASSO performs
selection by removing variables when coefficients shark to zero.

### Random Forest and Bagging Forest 
Random forest and Bagging forest generate a large number of bootstrapped
decision trees, which perform binary splits on data at decision nodes, which are
defined with a predictors’ levels. At each node, features are selected from a
random subsample of all features in the dataset. The feature that could split
the dataset in a way that minimizes RSS for the model is selected. Out of
Bag(OOB) score is used to validate random forest models. OOB samples are
left-outs of each bootstrapping sample, used as a testset. OOB score is computed
as the number of correct predictions, while OOB error rate is the number of
wrong classifications of OOBs. Variables in the model are assessed by variable
importance, which is penalized for large OOB error rate. A fundamental
difference between bagging forest and random forest is that at each node, all
features in the dataset are compared against each other. Therefore random forest
increases the speed of model building as well as the variety in the use of
features. Bagging forest is slower in model building speed and more restricted
in the use of features in split ting. On the other hand, it could bring higher
predictive accuracy, having better performance in minimizing MSE.

### Support Vector Machine
Support Vector Machine(SVM) was designed for classification, gand later the idea
was generalized to regression. SVM regression is used in this project. In SVM
regression, we would like to form a "band" around the true regression function
that contains most of the points. With this purpose, the loss function is
defined as below. 
\begin{align} 
\mu(x)=\beta_0 + x^T\beta 
\end{align} 
If the point (X,Y) = (x,y) is such that $|y-\mu(x)|\leq \epsilon$, then the loss
is taken to be zero; otherwise the loss is taken to be $|y-\mu(x)|- \epsilon$.
The main goal is to find a $\mu(x)$ such that most points with the loss taken to
be zero.

### Neural Network
Neural networks (NNs) are computing systems inspired by the way biological
neural networks in the human brain process information. The tasks to which
neural networks are applied tend to fall within the following broad categories
including regression analysis, classification, and data processing.

We adopt the most basic one: *Feedforward Neural Network* (FFNN). Feedforward
neural network was the first type of artificial neural network devised. It has
only one direction from input layer formed by input nodes, through hidden layers
and to out the output layer formed by output nodes. Specifically, we will adopt
*Multilayer Perceptron* (MLP). A multilayer perceptron consists of at least
three layers of nodes: an input layer, a hidden layer and an output layer. There
are 8 hidden layers in our model.

The toolkit we used for building the neural network is *Keras*. Keras is a deep
learning API (application programming interface) written in Python, running on
top of the machine learning platform *TensorFlow*.

## Dataset

### Data description

The raw data is from the UCI machine learning repository, which is a collection
of 8,760 bike rental records from 2017 to 2018. There are 14 variables in the
dataset. The table shows the features of each variable. The dataset contains
weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint,
Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and
date information.

### Exploratory data analysis

The data set included in the UCI has been processed, so there are no missing
data. We extracted day, month, and year, and converted date to the day of week.
The final rental bike data was partitioned into two: a training set for building
the regression and a testing set for assessing the model performance. 20% of the
8760 records were randomly selected as the test set and the other 80% as the
training set. The dimensions of the training and test sets are shown in Table 1.

Table 1. Training and test dataset

|Dataset      | Number of observations |
| :---        |    :-------------:     | 
|Training set | 7008 and 17 variables  |
|Test set     | 1752 and 17 variables  |
```{r, include=FALSE}
bike_data <- read.csv("data/SeoulBikeData.csv")

bike_data$Date = as.Date(bike_data$Date, "%d/%m/%Y")
bike_data$Year = factor(format(bike_data$Date, "%Y"))
bike_data$Month = factor(months(bike_data$Date))
bike_data$Day = factor(format(bike_data$Date, "%d"))
bike_data$DWeek = factor(weekdays(bike_data$Date))
bike_data$Seasons=as.factor(bike_data$Seasons)
bike_data$Functioning.Day=as.factor(bike_data$Functioning.Day)
bike_data$Holiday=as.factor(bike_data$Holiday)
colnames(bike_data) <- c('Date','Rented_Bike_Count', 'Hour', 'Temperature', 'Humidity', 'Wind_speed', 'Visibility', 'Dew_point_temp', 'Solar_Rad', 'Rainfall', 'Snowfall', 'Seasons', 'Holiday', 'Funct_Day','Year', 'Month', 'Day', 'DWeek')

library(ggplot2)
library(hrbrthemes)
library(ggthemes) 
library("PerformanceAnalytics")
library("psych")
library(corrplot)
library(RColorBrewer)
require(gridExtra)
library(ggcorrplot)
library("ggpubr")
```

Figure 1 shows the total number of rented bikes for the entire period. It
illustrates that the rental bike count is highly variable at each hour. As can
be seen, there is a long tail in the data distribution. Figure 2 displays an
average number of bike rentals by week days. It is shown that the count
distribution follows identical trends over the weekdays and different patterns
over the weekends. Figure 3 shows that the average count is high at each hour in
the summer and low in the winter. The count is quite similar during autumn and
spring. The count abruptly increases in hours 8 and 18. This is because the
hours from 8 AM and 6 PM are the peak hours, during which the usage of the
rental bike is high in Seoul.

```{r}
p1=ggplot(bike_data, aes(x=Rented_Bike_Count)) +
    geom_histogram(binwidth=110,
                   fill="lightblue", 
                   col="orange",
                   alpha = .8)+
  labs(title="Figure 1. Histogram for Rented Bike Count", x="Rented Bike Count", y="Count")+
  theme_stata() +
  scale_color_stata() +
  theme(plot.title = element_text(hjust = 0.5),legend.position = "None")+
  theme(axis.text=element_text(size=6),
        axis.title=element_text(size=4,face="bold"),plot.title = element_text(size = 10, face = "bold"))
  
# Plot Average count by week 
p2=ggplot(bike_data, aes(x=DWeek, y=Rented_Bike_Count ,fill= DWeek))+
  geom_bar(stat = "identity") +
  ggtitle("Figure 2. Average Count by Week")+
  xlab("Day of Week") + 
  ylab("Rent count")  + 
  labs(fill = "Day of Week") + 
  theme_stata() +
  scale_color_stata() +
  theme(plot.title = element_text(hjust = 0.5),legend.position = "None",,axis.text.x = element_text(angle=-25),axis.text.y = element_text(angle=-65))+
  theme(axis.text=element_text(size=6),
        axis.title=element_text(size=6,face="bold"),plot.title = element_text(size = 10, face = "bold"))
# Plot Average count vs hour by seasons
p3=ggplot(data=bike_data, aes(x = Hour, y = Rented_Bike_Count, colour = Seasons)) +
   stat_summary(geom = "line", fun = mean)+
   stat_summary(geom = "point", fun = mean)+
   ggtitle("Figure 3. Average Count vs Hour grouped by Seasons")+
  xlab("Hour") + 
  ylab("Rent count") +
  #theme_stata() +
  theme(plot.title = element_text(hjust = 0.5),legend.position = "None")+
  theme(axis.text=element_text(size=6),
        axis.title.x = element_text(size = 6),axis.title.y = element_text(size = 6),plot.title = element_text(size = 10, face = "bold"))
#plot correlation
my_data <- bike_data[, 2:11]
colnames(my_data)[c(1,3:10)] <- c('Count', 'Temp', 'Humiditity', 'Wind', 'Visb', 'Dew', 'Solar','Rain','Snow')
M <-cor(my_data)
p4=ggcorrplot(M,  type = "lower",
     outline.col = "white",lab = TRUE,lab_size= 1,tl.cex = 5,show.legend=FALSE)+
  labs(title = "Figure 4.Correlation Matrix ")+
  theme(axis.text=element_text(size=8),
        axis.title=element_text(size=4,face="bold"),plot.title = element_text(size = 10, face = "bold"))
ggarrange(p1,p2,p3,p4,nrow=2, ncol=2) 

```

As shown in Figure 4, the corplot function creates a graph of a correlation
matrix, coloring the regions according to the value correlation coefficients. A
correlation value of 1 is considered as a total positive correlation, $-1$ is
considered the total negative correlation, and if 0 no correlation exists
between the variables. Positive correlations are notable between count and hour,
temp, wind, visb, dew and solar. There is a negative correlation between count
and humidity, rain and snow. These correlation values imply that the weather
variables are related to the rental bike count at each hour.

## Model development
It is essential to tuning parameters in order to find the optimal model with
relatively low loss or error values.

For the SVM models, there are two tuning parameters, namely cost and sigma. The
first search for cost was within the range 1 to 100 and for sigma was from 0.01
to 0.1 are used. The optimal values are 20 and 0.1. Then we did a more specific
grid search around these two values. The optimal values sigma (0.11) and cost
(20) were selected as the best combination of parameters.

When building neural networks, we have tried different sizes of the hidden
layers. We believe that it is more likely to fit a model with more layers and
parameters. Hence we add the hidden layers from 5, the starting model, to 8, the
final optimal model. We also try different numbers of nodes ranging from 1 to
512 in different layers. There are more nodes in the first few hidden layers,
and the nodes number keeps reducing until the size of output (which is 1 since
we only need to make a prediction of the number of rented bike). The activation
function we adopted are ReLU and Linear. Rectified Linear Unit (ReLU) is an
activation function that defines the positive part of its arguement. It can be
expressed as:
\begin{align}
f(x) = x^{+} = \max(0, x)
\end{align}
Linear is an activation function that simply returns the the argument itself. We
also add regularizers that apply to a L1 and L2 penalty on the layer's output.
L1 penalty is the penalty term in L1 regression or Lasso regression. L2 penalty
is the penalty term in L2 regression or Ridge regression. Regularizers are used
to avoid model overfitting and improve the robustness and generality. By setting
the batch size to 64, we run 110 iterations per epoch with over 800 epochs for
the optimal model. A large number of parameters may also bring the problem of
overfitting. In the figure below, we can tell that the train and test RMSE have
clear trends of decreasing, except for the unusual high test RMSE in model 5.
This could be attributed to overfitting. To solve this problem, we constraint
the complexity of the model. No more than 8 hidden layers and no more than 512
number of nodes in each layers. Typically, we adopt 256 or 64 nodes per layer.
Furthermore, we introduce regularizers that add L1 or L2 penalty to the model
during training base on the magnitude of the activation. We also try to randomly
drop out a portion (i.e. 20%) of the input in specific layer while training. But
later we find thfat the performance of these models is not as good as the one
without dropping the input.

\begin{figure}
  \centering
  \includegraphics[height=4cm,width=6cm]{graphs/nn_mod_mse.png}
  \includegraphics[height=4cm,width=6cm]{graphs/nn_models_RMSE_Compare.png}
\end{figure}
<!-- ![History of Train and Validation MSE](graphs/nn_mod_mse.png)  -->
<!-- ![RMSE Comparison between Models](graphs/nn_models_RMSE_Compare.png) -->

## Result
We did 10-fold cross-validation in model training, and used the average of 10
outcomes to be the RMSE of each training model. Table 2. shows the RMSE
value of the trained regression models both in testing and training sets. As we
can see, the NN model has the lowest RMSE in both the train set and test set.
The bagging Forest model (BagF) also has an excellent performance among the
models of basic machine learning methods. The models of LM, LASSO and Ridge
produce the worst results compared to other models. The multicollinearity of
covariates may be the main cause the unsatisfying results.

Table 2. RMSE values of training set and testing set
 Method | Training | Testing
   :-  |   :-:    |   :-:
 LM     | 407.0000 | 417.0000
 Ridge  | 427.1653 | 437.7671
 LASSO  | 427.3688 | 437.4730
 RF     | 224.6018 | 222.9583
 BagF   | 184.1149 | 182.7463
 SVM    | 226.5303 | 221.5075
 NN     | 120.0056 | 133.2593

## Discussion
Ridge and Lasso are used when n is not much larger than p, as there can be a lot
of variability in the fit, which can result in either overfitting and very poor
predictive ability. Our training set has 7008 observations of 17 variables,
which implies we are in the case n»p. Therefore, we don’t have the problem that
Ridge and Lasso intended to solve. Furthermore, variable selection (like in
Ridge and Lasso) is useful when some predictors are not significant or
predictors are highly correlated. In our dataset, most predictors are highly
significant, correlation between them is moderate, and VIFs aren’t exaggerated.
Thus, other circumstances that could make variable selection useful don’t hold
here. This explains why Lasso and Ridge regression didn’t perform well in our
project.

Both bagging forest and random forest show that Hour and Functioning_Day are
significantly more important than other variables in predicting rental count.
Bagging forest results in smaller RMSE than random forest, suggesting that since
the number of variables is not excessive, increasing variety during feature
selection is not needed for this dataset.

Random seed setting may lead to different results in neural networks. When
setting the seed as *625*, the difference between validation mse and test mse is
large, indicating that the data split is not even. Validation and test mse are
getting close when changing the seed as *48107*.

## Citation

E, S., Park, J., &amp; Cho, Y. (2020). Using data mining techniques for bike
sharing demand prediction in Metropolitan City. Computer Communications, 153,
353–366. https://doi.org/10.1016/j.comcom.2020.02.007

Izenman, A. J. (2008). Modern multivariate statistical techniques. Regression,
classification and manifold learning, 10, 978-0.
